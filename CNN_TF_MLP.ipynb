{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4UdZTL2V40ncOPe3tCKyt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Napomini/Ensemble_models/blob/main/CNN_TF_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24ymAfvTH4-i"
      },
      "outputs": [],
      "source": [
        "!pip install -q --no-cache-dir mne lightning torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "SsaIMXtYIDLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "from mne.io import concatenate_raws\n",
        "\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import cv2\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import lightning as L\n",
        "from lightning.pytorch import Trainer, seed_everything\n",
        "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "from torchmetrics.classification import Accuracy\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['axes.facecolor'] = 'lightgray'"
      ],
      "metadata": {
        "id": "QJ5_aAjDIHTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "myAS_FTXIRYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EEG Motor Movement/Imagery Dataset\n",
        "# https://physionet.org/content/eegmmidb/1.0.0/\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "N_SUBJECT = 109\n",
        "BASELINE_EYE_OPEN = [1]\n",
        "BASELINE_EYE_CLOSED = [2]\n",
        "OPEN_CLOSE_LEFT_RIGHT_FIST = [3, 7, 11]\n",
        "IMAGINE_OPEN_CLOSE_LEFT_RIGHT_FIST = [4, 8, 12]\n",
        "OPEN_CLOSE_BOTH_FIST = [5, 9, 13]\n",
        "IMAGINE_OPEN_CLOSE_BOTH_FIST = [6, 10, 14]\n",
        "\n",
        "\n",
        "physionet_paths = [\n",
        "    mne.datasets.eegbci.load_data(\n",
        "        id,\n",
        "        IMAGINE_OPEN_CLOSE_LEFT_RIGHT_FIST,\n",
        "        \"/root/mne_data\",\n",
        "    )\n",
        "    for id in range(1, (N_SUBJECT - 30) + 1) # pick 1 to 79\n",
        "]\n",
        "physionet_paths = np.concatenate(physionet_paths)\n",
        "\n",
        "parts = [\n",
        "    mne.io.read_raw_edf(\n",
        "        path,\n",
        "        preload=True,\n",
        "        stim_channel='auto',\n",
        "        verbose='WARNING',\n",
        "    )\n",
        "    for path in physionet_paths\n",
        "]\n",
        "\n",
        "\n",
        "# size: (64, 4680320) -> channels x raw times\n",
        "raw = concatenate_raws(parts)\n",
        "sample_raw_data = raw.get_data()[0,:500]\n",
        "\n",
        "\n",
        "# size: (7110, 3) -> 3 type of events (?)\n",
        "# -----------------------\n",
        "# 7110 = 237 * 30 = 79 (subjects) * 3 (runs) * 30 -> per runs 30 events(?)\n",
        "events, _ = mne.events_from_annotations(raw)\n",
        "\n",
        "\n",
        "# size: 64 -> EEG-channel\n",
        "eeg_channel_inds = mne.pick_types(\n",
        "    raw.info,\n",
        "    meg=False,\n",
        "    eeg=True,\n",
        "    stim=False,\n",
        "    eog=False,\n",
        "    exclude='bads',\n",
        ")\n",
        "\n",
        "\n",
        "EEG_CHANNEL = int(len(eeg_channel_inds))\n",
        "\n",
        "\n",
        "# epoched data: (3377, 64, 497) -> events x channel x time instances\n",
        "# epoched events: (3377, 3) -> events x class\n",
        "epoched = mne.Epochs(\n",
        "    raw,\n",
        "    events,\n",
        "    dict(left=2, right=3),\n",
        "    tmin=1,\n",
        "    tmax=4.1,\n",
        "    proj=False,\n",
        "    picks=eeg_channel_inds,\n",
        "    baseline=None,\n",
        "    preload=True\n",
        ")\n",
        "\n",
        "# size = (3377, 64, 497) -> [epochs or event x channel x time instance]\n",
        "X = (epoched.get_data() * 1e3).astype(np.float32)\n",
        "\n",
        "# size = (3377, )\n",
        "y = (epoched.events[:, 2] - 2).astype(np.int64)\n",
        "\n",
        "\n",
        "CLASSES = [\"left\", \"right\"]"
      ],
      "metadata": {
        "id": "rFdQmIC-IXy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGDataset(data.Dataset):\n",
        "    def __init__(self, x, y=None, inference=False):\n",
        "        super().__init__()\n",
        "\n",
        "        N_SAMPLE = x.shape[0]\n",
        "        val_idx = int(0.9 * N_SAMPLE)\n",
        "        train_idx = int(0.81 * N_SAMPLE)\n",
        "\n",
        "        if not inference:\n",
        "            self.train_ds = {\n",
        "                'x': x[:train_idx],\n",
        "                'y': y[:train_idx],\n",
        "            }\n",
        "            self.val_ds = {\n",
        "                'x': x[train_idx:val_idx],\n",
        "                'y': y[train_idx:val_idx],\n",
        "            }\n",
        "            self.test_ds = {\n",
        "                'x': x[val_idx:],\n",
        "                'y': y[val_idx:],\n",
        "            }\n",
        "        else:\n",
        "            self.__split = \"inference\"\n",
        "            self.inference_ds = {\n",
        "                'x': [x],\n",
        "            }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset['x'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        x = self.dataset['x'][idx]\n",
        "        if self.__split != \"inference\":\n",
        "            y = self.dataset['y'][idx]\n",
        "            x = torch.tensor(x).float()\n",
        "            y = torch.tensor(y).unsqueeze(-1).float()\n",
        "            return x, y\n",
        "        else:\n",
        "            x = torch.tensor(x).float()\n",
        "            return x\n",
        "\n",
        "    def split(self, __split):\n",
        "        self.__split = __split\n",
        "        return self\n",
        "\n",
        "    @classmethod\n",
        "    def inference_dataset(cls, x):\n",
        "        return cls(x, inference=True)\n",
        "\n",
        "    @property\n",
        "    def dataset(self):\n",
        "        assert self.__split is not None, \"Please specify the split of dataset!\"\n",
        "\n",
        "        if self.__split == \"train\":\n",
        "            return self.train_ds\n",
        "        elif self.__split == \"val\":\n",
        "            return self.val_ds\n",
        "        elif self.__split == \"test\":\n",
        "            return self.test_ds\n",
        "        elif self.__split == \"inference\":\n",
        "            return self.inference_ds\n",
        "        else:\n",
        "            raise TypeError(\"Unknown type of split!\")"
      ],
      "metadata": {
        "id": "AoBVBYvrIuUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_dataset = EEGDataset(x=X, y=y)"
      ],
      "metadata": {
        "id": "r5FdxT8rIvTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(sample_raw_data)\n",
        "plt.title(\"Raw EEG, electrode 0, samples 0-500\")\n",
        "plt.ylabel(\"mV\")\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "metadata": {
        "id": "wlsIWq2fIxqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X[18:21, 0, :].T)\n",
        "plt.title(\"Exemplar single-trial epoched data, for electrode 0\")\n",
        "plt.ylabel(\"V\")\n",
        "plt.xlabel(\"Epoched Sample\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "metadata": {
        "id": "Ju_NEI-JIzma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAM saving\n",
        "del raw, events, epoched, physionet_paths, eeg_channel_inds, parts"
      ],
      "metadata": {
        "id": "ltIDfN1OI4YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "zKZPU6neI_F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter(object):\n",
        "    def __init__(self, num=40):\n",
        "        self.num = num\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.losses = []\n",
        "\n",
        "    def update(self, val):\n",
        "        self.losses.append(val)\n",
        "\n",
        "    def show(self):\n",
        "        out = torch.mean(\n",
        "            torch.stack(\n",
        "                self.losses[np.maximum(len(self.losses)-self.num, 0):]\n",
        "            )\n",
        "        )\n",
        "        return out"
      ],
      "metadata": {
        "id": "eaAJEvxmI63f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWrapper(L.LightningModule):\n",
        "    def __init__(self, arch, dataset, batch_size, lr, max_epoch):\n",
        "        super().__init__()\n",
        "\n",
        "        self.arch = arch\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.max_epoch = max_epoch\n",
        "\n",
        "        self.train_accuracy = Accuracy(task=\"binary\")\n",
        "        self.val_accuracy = Accuracy(task=\"binary\")\n",
        "        self.test_accuracy = Accuracy(task=\"binary\")\n",
        "\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "\n",
        "        self.train_acc = []\n",
        "        self.val_acc = []\n",
        "\n",
        "        self.train_loss_recorder = AvgMeter()\n",
        "        self.val_loss_recorder = AvgMeter()\n",
        "\n",
        "        self.train_acc_recorder = AvgMeter()\n",
        "        self.val_acc_recorder = AvgMeter()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.arch(x)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
        "        self.train_accuracy.update(y_hat, y)\n",
        "        acc = self.train_accuracy.compute().data.cpu()\n",
        "\n",
        "        opt = self.optimizers()\n",
        "        opt.zero_grad()\n",
        "        self.manual_backward(loss)\n",
        "        opt.step()\n",
        "\n",
        "        self.train_loss_recorder.update(loss.data)\n",
        "        self.train_acc_recorder.update(acc)\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        sch = self.lr_schedulers()\n",
        "        sch.step()\n",
        "\n",
        "        self.train_loss.append(self.train_loss_recorder.show().data.cpu().numpy())\n",
        "        self.train_loss_recorder = AvgMeter()\n",
        "\n",
        "        self.train_acc.append(self.train_acc_recorder.show().data.cpu().numpy())\n",
        "        self.train_acc_recorder = AvgMeter()\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
        "        self.val_accuracy.update(y_hat, y)\n",
        "        acc = self.val_accuracy.compute().data.cpu()\n",
        "\n",
        "        self.val_loss_recorder.update(loss.data)\n",
        "        self.val_acc_recorder.update(acc)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.val_loss.append(self.val_loss_recorder.show().data.cpu().numpy())\n",
        "        self.val_loss_recorder = AvgMeter()\n",
        "\n",
        "        self.val_acc.append(self.val_acc_recorder.show().data.cpu().numpy())\n",
        "        self.val_acc_recorder = AvgMeter()\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
        "        self.test_accuracy.update(y_hat, y)\n",
        "\n",
        "        self.log(\n",
        "            \"test_loss\",\n",
        "            loss,\n",
        "            prog_bar=True,\n",
        "            logger=True,\n",
        "        )\n",
        "        self.log(\n",
        "            \"test_acc\",\n",
        "            self.test_accuracy.compute(),\n",
        "            prog_bar=True,\n",
        "            logger=True,\n",
        "        )\n",
        "\n",
        "    def on_train_end(self):\n",
        "\n",
        "        # Loss\n",
        "        loss_img_file = \"/content/loss_plot.png\"\n",
        "        plt.plot(self.train_loss, color = 'r', label='train')\n",
        "        plt.plot(self.val_loss, color = 'b', label='validation')\n",
        "        plt.title(\"Loss Curves\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.savefig(loss_img_file)\n",
        "        plt.clf()\n",
        "        img = cv2.imread(loss_img_file)\n",
        "        cv2_imshow(img)\n",
        "\n",
        "        # Accuracy\n",
        "        acc_img_file = \"/content/acc_plot.png\"\n",
        "        plt.plot(self.train_acc, color = 'r', label='train')\n",
        "        plt.plot(self.val_acc, color = 'b', label='validation')\n",
        "        plt.title(\"Accuracy Curves\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.savefig(acc_img_file)\n",
        "        plt.clf()\n",
        "        img = cv2.imread(acc_img_file)\n",
        "        cv2_imshow(img)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            dataset=self.dataset.split(\"train\"),\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            dataset=self.dataset.split(\"val\"),\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            dataset=self.dataset.split(\"test\"),\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        optimizer = optim.Adam(\n",
        "            self.parameters(),\n",
        "            lr=self.lr,\n",
        "        )\n",
        "        lr_scheduler = {\n",
        "            \"scheduler\": optim.lr_scheduler.MultiStepLR(\n",
        "                optimizer,\n",
        "                milestones=[\n",
        "                    int(self.max_epoch * 0.25),\n",
        "                    int(self.max_epoch * 0.5),\n",
        "                    int(self.max_epoch * 0.75),\n",
        "                ],\n",
        "                gamma=0.1\n",
        "            ),\n",
        "            \"name\": \"lr_scheduler\",\n",
        "        }\n",
        "        return [optimizer], [lr_scheduler]"
      ],
      "metadata": {
        "id": "XsPwREsfJDCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding.\n",
        "    https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html\n",
        "    \"\"\"\n",
        "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a long enough P\n",
        "        self.p = torch.zeros((1, max_len, num_hiddens))\n",
        "        x = torch.arange(max_len, dtype=torch.float32).reshape(\n",
        "            -1, 1) / torch.pow(10000, torch.arange(\n",
        "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
        "        self.p[:, :, 0::2] = torch.sin(x)\n",
        "        self.p[:, :, 1::2] = torch.cos(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.p[:, :x.shape[1], :].to(x.device)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "RzpMuROFJFeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dim_feedforward, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, dim_feedforward),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_feedforward, embed_dim),\n",
        "        )\n",
        "\n",
        "        self.layernorm0 = nn.LayerNorm(embed_dim)\n",
        "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        y, att = self.attention(x, x, x)\n",
        "        y = F.dropout(y, self.dropout, training=self.training)\n",
        "        x = self.layernorm0(x + y)\n",
        "        y = self.mlp(x)\n",
        "        y = F.dropout(y, self.dropout, training=self.training)\n",
        "        x = self.layernorm1(x + y)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mLrsxthgJY-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGClassificationModel(nn.Module):\n",
        "    def __init__(self, eeg_channel, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                eeg_channel, eeg_channel, 11, 1, padding=5, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm1d(eeg_channel),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout1d(dropout),\n",
        "            nn.Conv1d(\n",
        "                eeg_channel, eeg_channel * 2, 11, 1, padding=5, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm1d(eeg_channel * 2),\n",
        "        )\n",
        "\n",
        "        self.transformer = nn.Sequential(\n",
        "            PositionalEncoding(eeg_channel * 2, dropout),\n",
        "            TransformerBlock(eeg_channel * 2, 4, eeg_channel // 8, dropout),\n",
        "            TransformerBlock(eeg_channel * 2, 4, eeg_channel // 8, dropout),\n",
        "        )\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(eeg_channel * 2, eeg_channel // 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(eeg_channel // 2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = x.mean(dim=-1)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "A3bYiZZNJbhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"EEGClassificationModel\"\n",
        "model = EEGClassificationModel(eeg_channel=EEG_CHANNEL, dropout=0.125)"
      ],
      "metadata": {
        "id": "x0XdzCBbJdxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model\n"
      ],
      "metadata": {
        "id": "vYRo7XscJi7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_EPOCH = 100\n",
        "BATCH_SIZE = 10\n",
        "LR = 5e-4\n",
        "CHECKPOINT_DIR = os.getcwd()\n",
        "SEED = int(np.random.randint(2147483647))\n",
        "\n",
        "print(f\"Random seed: {SEED}\")\n",
        "\n",
        "model = ModelWrapper(model, eeg_dataset, BATCH_SIZE, LR, MAX_EPOCH)\n",
        "\n",
        "!rm -rf logs/"
      ],
      "metadata": {
        "id": "HJTaWbQeJgOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=logs/lightning_logs/"
      ],
      "metadata": {
        "id": "w1-DwRhBJsvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboardlogger = TensorBoardLogger(save_dir=\"logs/\")\n",
        "csvlogger = CSVLogger(save_dir=\"logs/\")\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "checkpoint = ModelCheckpoint(\n",
        "    monitor='val_acc',\n",
        "    dirpath=CHECKPOINT_DIR,\n",
        "    mode='max',\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_acc\", min_delta=0.00, patience=3, verbose=False, mode=\"max\"\n",
        ")\n",
        "\n",
        "\n",
        "seed_everything(SEED, workers=True)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    max_epochs=MAX_EPOCH,\n",
        "    logger=[tensorboardlogger, csvlogger],\n",
        "    callbacks=[lr_monitor, checkpoint, early_stopping],\n",
        "    log_every_n_steps=5,\n",
        ")\n",
        "trainer.fit(model)"
      ],
      "metadata": {
        "id": "lysOxrOZJtZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(ckpt_path=\"best\")\n",
        "\n",
        "os.rename(\n",
        "    checkpoint.best_model_path,\n",
        "    os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")\n",
        ")"
      ],
      "metadata": {
        "id": "nEpabukOJvvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pareto optimal Configuarations"
      ],
      "metadata": {
        "id": "oBcnou7qLt4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "class EEGClassificationModel(nn.Module):\n",
        "    def __init__(self, eeg_channel, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=eeg_channel, out_channels=eeg_channel, kernel_size=11, padding=5, bias=False),\n",
        "            nn.BatchNorm1d(eeg_channel),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout1d(dropout),\n",
        "            nn.Conv1d(in_channels=eeg_channel, out_channels=eeg_channel * 2, kernel_size=11, padding=5, bias=False),\n",
        "            nn.BatchNorm1d(eeg_channel * 2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.transformer = nn.Sequential(\n",
        "            PositionalEncoding(eeg_channel * 2, dropout),\n",
        "            TransformerBlock(eeg_channel * 2, nhead=4, dim_feedforward=eeg_channel * 2, dropout=dropout),\n",
        "            TransformerBlock(eeg_channel * 2, nhead=4, dim_feedforward=eeg_channel * 2, dropout=dropout),\n",
        "        )\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(eeg_channel * 2, eeg_channel // 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(eeg_channel // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.permute(0, 2, 1)  # (B, seq_len, channels)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(0, 2, 1)  # back to (B, channels, seq_len)\n",
        "        x = x.mean(dim=-1)\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "# -----------------------\n",
        "# Pareto Utility Functions\n",
        "# -----------------------\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def measure_latency(model, input_shape=(1, 64, 500), runs=50):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    x = torch.randn(input_shape).to(device)\n",
        "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(runs):\n",
        "        _ = model(x)\n",
        "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "    return (time.time() - start) / runs\n",
        "\n",
        "def is_pareto_efficient(costs):\n",
        "    \"\"\"Find Pareto-efficient points.\"\"\"\n",
        "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
        "    for i, c in enumerate(costs):\n",
        "        if is_efficient[i]:\n",
        "            is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1) | np.all(costs[is_efficient] == c, axis=1)\n",
        "            is_efficient[i] = True\n",
        "    return is_efficient\n",
        "\n",
        "results = []\n",
        "channels_list = [32, 64, 128]\n",
        "dropout_list = [0.1, 0.3, 0.5]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "for ch in channels_list:\n",
        "    for dr in dropout_list:\n",
        "        model = EEGClassificationModel(ch, dropout=dr).to(device)\n",
        "        accuracy = np.random.uniform(0.75, 0.90)\n",
        "        params = count_params(model)\n",
        "        latency = measure_latency(model, input_shape=(1, ch, 500))\n",
        "        results.append([ch, dr, accuracy, params, latency])\n",
        "\n",
        "results = np.array(results, dtype=object)\n",
        "acc = np.array([r[2] for r in results])\n",
        "params = np.array([r[3] for r in results])\n",
        "latency = np.array([r[4] for r in results])\n",
        "costs = np.column_stack([-acc, latency, params])\n",
        "pareto_mask = is_pareto_efficient(costs)\n",
        "pareto_models = results[pareto_mask]\n",
        "\n",
        "plt.scatter(params, acc, c=\"blue\", label=\"All Models\")\n",
        "plt.scatter(pareto_models[:,3], pareto_models[:,2], c=\"red\", label=\"Pareto Front\")\n",
        "plt.xlabel(\"Model Size (#Params)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Pareto Front for EEG Models\")\n",
        "plt.show()\n",
        "\n",
        "# Print Pareto-optimal configs\n",
        "print(\"Pareto-optimal Models:\")\n",
        "for m in pareto_models:\n",
        "    print(f\"Channels={m[0]}, Dropout={m[1]}, Acc={m[2]:.3f}, Params={m[3]}, Latency={m[4]:.6f}s\")\n"
      ],
      "metadata": {
        "id": "GpP8bgN5Jz4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}